# 什么是DETR？DETR的前世今生——小伞学AI



第一章：从注意力说起......

- 我们在与这个世界交互的过程中，总是会优先“注意”一些事情，举个例子，当你很饿的时候，对于你脸上摆放的一个桃与一本书，显然，这时候你会更加“注意”那本桃。而不是一本不能果腹的书。而当你肚子很饱很饱的时候，你会更加注意那本书，而不是那个桃（吃不下了！！！）。在这个场景，你拥有一系列状态，例如肚子饱不饱，身高，体重，头发量......这些我们统称为“状态输入”，而“注意力”就是根据这些“状态输入”，从而决定我们的下一个行动（输出）的机制。
- 在AI业界，注意力的概念同样广泛流传，AI归根到底就是信息的转换器，我们通过AI，将输入的信息中“我们需要注意的内容”提取出来，或者是处理后得到“我们想要注意的答案”。
- 在早期的AI界，当年还是机器学习的时代，最早的注意力工作是由人工完成的——即为特征工程，3学家在处理数据的时候会将那些“没有用的”“缺失的”“明显有问题的”......数据进行处理或者直接删掉——这就是最早的“AI注意力机制”了，它以人作为算法（世界上最强大的AI（doge）），将状态输入（数据，进行处理，从而得到“需要被注意的信息”），优点是算法（人）本身十分强大，可以对数据进行高度有效的操作，但缺点是该算法的处理效率奇慢，而且有时候会闹情绪，导致时间复杂度飙升（不是）。
- 同时，我们用的一次函数的算法也可以理解为一种注意力机制，
- 到后来，科学家们采用“卷积方法”作为一种局部的注意力机制。通过对图像数据进行卷积处理，本质上相当于对数据做了一个稀疏高重复的矩阵乘法（公式）。结合训练方法（反向传播等）修改卷积的参数，从而使卷积能够注意图像整体的轮廓信息，这样，经过多轮的卷积操作，图像的轮廓纹理信息被提取处理，模型根据这些被注意到的信息做出判断，这是一种“局部的注意力机制”。
- 然后根据卷积方法衍生出了一系列用于处理各种信息的模型，统称为卷积模型。
- 再往后人们发现卷积这种算法是有局限的，于是（我不做卷积啦，JOJO！）就有了注意力机制，归根到底也可以表示为一个线性操作，但是比起卷积更有效（芙西米，互搂互吸，四蛋都怕我！）（准确来讲，是attention相较于卷积**在高密度的信息上更有优势**），在有业界大佬证明了注意力机制在图像领域的有效性后，同上，一批优质模型雨后春笋，DETR就是其中之一。
- 本章介绍就到这里了，下一章我们讲解一下解析detr项目所需要的**算法知识。**



第二章：关于一堆方块与AI的故事



* 废案
  * 本章我们来讲解一下解析detr的前置知识——矩阵运算与AI的相关知识
  * 等等等别急着划走呀，继续往下看看？
  * 这是一个线，我们叫他一维
  * 现在，假设这不是一个普通的线，它是压缩线，遇水变大变宽，(๑•̀ㅂ•́)و✧它是一个平面，我们叫它二维数据
  * 假设它不是一个普通的平面，它遇水变大变高，(๑•̀ㅂ•́)و✧现在它叫三维了。
  * 假设它——好了，接下来就是四维了，但四维数据不好想象怎么办？
  * 所以，它遇水直接分裂！(๑•̀ㅂ•́)و✧现在这一堆就是四维数据了，有几个方块第四维就有多大。
* 阅读本文前建议具备数组与矩阵的相关知识
* 首先，AI需要吃名为数据的方块，方块是一个固定维数形状的矩阵，如果维数不固定它不吃!
* 然后，想象一堆方块，这些方块的长宽高都是相同的，现在我用一个数组来放数据：【长，宽，高，方块个数】——这就可以代指四维的数据形状了（实在想象不出来的妙招）
* 我们手上有一堆名为“图像”的方块——长宽都一样，具备RGB——3个通道色值，所以把这些数值放到一个数组，【颜色个数，长，宽】，就是图像了。
* 再者，由于一个方块一个方块的喂给AI太慢，我们采用批量投喂，每次喂n个方块，数组变成【n，颜色个数，长，宽】，这就是一个四维的batch（批次）图像数据。
* 我们要投喂给DETR的就是这种小方块。
* 接下来我们展示方块在DETR肚子内的消化过程
* 首先我们投喂了一个批次图像四维方块——形状为【2，3，200，250】
* 这个方块进入DETR后会先到达已经预训练好的ResNet内

